{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673781e7-0302-42d2-a8b6-4bfa89262d5a",
   "metadata": {},
   "source": [
    "# Consolidation Notebook\n",
    "\n",
    "Oct 9, 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6af51-b316-44f8-8a97-9ebeb51d7b9d",
   "metadata": {},
   "source": [
    "## Import & Preparation of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97f3cca-10bf-48f5-95e4-5a4727cac17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TnesorFlow version: 2.4.1\n",
      "TnesorFlow Probability version: 0.12.1\n",
      "WARNING: GPU device not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 07:18:14.941468: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import probflow as pf\n",
    "\n",
    "# Data Viz. \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import seaborn as sns\n",
    "sns.set_style(\n",
    "    style='darkgrid', \n",
    "    rc={'axes.facecolor': '.9', 'grid.color': '.8'}\n",
    ")\n",
    "sns.set_palette(palette='deep')\n",
    "sns_c = sns.color_palette(palette='deep')\n",
    "%matplotlib inline\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "dtype = 'float32'\n",
    "\n",
    "# Get TensorFlow version.\n",
    "print(f'TnesorFlow version: {tf.__version__}')\n",
    "print(f'TnesorFlow Probability version: {tfp.__version__}')\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "  print('WARNING: GPU device not found.')\n",
    "else:\n",
    "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ef073-57b1-476a-a197-e45b6d9582e2",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87054d24-c6ab-44cc-b893-58e829c32322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:   (192, 2) (192,)  test:   (95, 2) (95,)\n",
      "X_t shape (192, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 07:18:29.160877: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-13 07:18:29.163440: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 32. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (163, 2), y shape: (163, 1)\n"
     ]
    }
   ],
   "source": [
    "xname=\"sm_0.67\"; yname=\"halo_mass\"\n",
    "xname=\"sm_1.0\"; yname=\"halo_mass\"; x2name=\"central_sm\"\n",
    "\n",
    "gal_df = pd.read_csv(\"Data/galaxies_near_clusters_0.3-0.6.csv\") \n",
    "cluster_data = pd.read_csv(\"Data/cluster_data_0.3-0.6.csv\")\n",
    "x=cluster_data[xname]; \n",
    "y=cluster_data[yname];\n",
    "x2=cluster_data[x2name]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Set seed.\n",
    "tf.random.set_seed(42)\n",
    "# Set tensor numeric type.\n",
    "dtype = 'float32'\n",
    "\n",
    "scaler = StandardScaler()\n",
    "detector = IsolationForest(n_estimators=1000,  contamination=\"auto\", random_state=0)\n",
    "\n",
    "# split into train and test sets\n",
    "X=np.vstack([x,x2]).transpose()\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X, y, train_size=0.67)\n",
    "print(\"train:  \",X_train.shape, y_train.shape,\" test:  \",  X_test.shape, y_test.shape)\n",
    "#print(X_train[:,0].shape, X_train[:,1].shape,y_train.shape)\n",
    " \n",
    "# Scaling\n",
    "unit_df =pd.DataFrame(data={xname:X_train[:,0], x2name:X_train[:,1], yname:y_train})\n",
    "# Scale data to zero mean and unit variance.\n",
    "X_t = scaler.fit_transform(unit_df)\n",
    "print(\"X_t shape\",X_t.shape)\n",
    "\n",
    "# Remove outliers.\n",
    "detector = IsolationForest(n_estimators=1000,  contamination=0.15, random_state=0)\n",
    "is_inlier = detector.fit_predict(X_t)\n",
    "X_t = X_t[(is_inlier > 0),:]\n",
    "unit_df =pd.DataFrame(data={xname:X_t[:,0], x2name:X_t[:,1], yname:X_t[:,2]})\n",
    "\n",
    "# Inverse scaling\n",
    "X_t = scaler.inverse_transform(unit_df)\n",
    "inv_df=pd.DataFrame(data={xname:X_t[:,0], x2name:X_t[:,1], yname:X_t[:,2]})\n",
    "xc=X_t[:,0]\n",
    "x2c=X_t[:1]\n",
    "yc=X_t[:,2]\n",
    "\n",
    "# Tensors for tensorflow\n",
    "# all the data\n",
    "x = np.vstack([cluster_data[xname],cluster_data[x2name]]).transpose()\n",
    "x = tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "y = tf.convert_to_tensor(cluster_data[yname], dtype=dtype)\n",
    "y = tf.reshape(y, (-1, 1))\n",
    "\n",
    "# Just the train data\n",
    "x = np.vstack([inv_df[xname],inv_df[x2name]]).transpose()\n",
    "x = tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "y = tf.convert_to_tensor(inv_df[yname], dtype=dtype)\n",
    "y = tf.reshape(y, (-1, 1))\n",
    "\n",
    "print(\"x shape: {}, y shape: {}\".format(x.shape,y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa0447-c7d6-46b6-aa17-858d1711746a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model libraries\n",
    "\n",
    "### Linear fit\n",
    "A linear regression can be expressed as model with only a single perceptron tf.keras.layers.Dense(1), which by default has a linear activation function and can take bias into account (that is, for x=0 the model will not necessarily predict y=0).\n",
    "\n",
    "- model = tf.keras.Sequential([    tf.keras.layers.Dense(1)  ])\n",
    "\n",
    "### Gaussians for each point\n",
    "\n",
    "Let’s try now to make the model output not just a number, but a distribution; for the sake of definiteness, let’s try to learn the “best” normal distribution which models the random variable Y for each value of X . In order to do so, notice that we cannot use anymore the mean squared error as a loss function, since it is not a meaningful error metric in this case. We will, instead, use the negative logarithm of likelihood between the distributions given by our model vs the observed data; in this way, training our model becomes essentially a maximum likelihood estimation of parameters!\n",
    "\n",
    "Notice also that for each value of X the normal distribution we want our model to learn is parametrized by two numbers: mean and standard deviation. Therefore, we should increase the number of neurons in the dense layer from one to two, so that each perceptron will learn the functional dependency on X of one of the two parameters, and then push its output into the distribution.\n",
    "\n",
    "The way to implement these changes in TensorFlow Probability is very nice: we can use a tfp.layers.DistributionLambda layer which works in pretty much the same way as a “standard” Keras layer; in its argument, we can plug a lambda function which takes parameters from the previous layers of the network and returns a tfp.Distribution:\n",
    "\n",
    "- model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(2),\n",
    "            tfp.layers.DistributionLambda(lambda t:\n",
    "                tfp.distributions.LogNormal( loc   = t[...,:1], scale = tf.math.softplus(0.005*t[...,1:])+0.001))   ])\n",
    "                \n",
    "- negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "\n",
    "### Case 1: No Uncertainty\n",
    "\n",
    "Seems like the above. The argument to Dense() is units: Positive integer, dimensionality of the output space.\n",
    "\n",
    "-  model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(1),\n",
    "          tfp.layers.DistributionLambda(lambda t: \n",
    "              tfd.Normal(loc=t, scale=1)), ])\n",
    "\n",
    "### Case 2: Aleatoric Uncertainty\n",
    "\n",
    "- model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(1 + 1),\n",
    "          tfp.layers.DistributionLambda(lambda t: \n",
    "              tfd.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.05 * t[...,1:]))),])\n",
    "              \n",
    "### Case 3: Epistemic Uncertainty\n",
    "\n",
    "- model = tf.keras.Sequential([\n",
    "          tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/x.shape[0]),\n",
    "          tfp.layers.DistributionLambda(lambda t: \n",
    "              tfd.Normal(loc=t, scale=1)),])\n",
    "plus models for the prior and posterior\n",
    "\n",
    "- def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "      n = kernel_size + bias_size\n",
    "      c = np.log(np.expm1(1.))\n",
    "      return tf.keras.Sequential([\n",
    "          tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "          tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "              tfd.Normal(loc=t[..., :n],\n",
    "                     scale=1e-5 + tf.nn.softplus(c + t[..., n:])), reinterpreted_batch_ndims=1)),])\n",
    "  \n",
    "- def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "      n = kernel_size + bias_size\n",
    "      return tf.keras.Sequential([\n",
    "          tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "          tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "              tfd.Normal(loc=t, scale=1), reinterpreted_batch_ndims=1)),])\n",
    "              \n",
    "### Case 4: Aleatoric & Epistemic Uncertainty\n",
    "\n",
    "- model = tf.keras.Sequential([\n",
    "          tfp.layers.DenseVariational(1 + 1, posterior_mean_field, prior_trainable, kl_weight=1/x.shape[0]),\n",
    "          tfp.layers.DistributionLambda(lambda t: \n",
    "              tfd.Normal(loc=t[..., :1],\n",
    "                  scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]))),])\n",
    "                  \n",
    "\n",
    "### Aleatoric Uncertainty (bnn2-for-stellarmass)\n",
    "\n",
    "To account for aleotoric uncertainty, which arises from the noise in the output, dense layers are combined with probabilistic layers. More specifically, the mean and covariance matrix of the output is modelled as a function of the input and parameter weights. \n",
    "\n",
    "The first hidden layer shall consist of ten nodes, the second one needs four nodes for the means plus ten nodes for the variances and covariances of the four-dimensional (there are four outputs) multivariate Gaussian posterior probability distribution in the final layer. This is achieved using the params_size method of the last layer (MultivariateNormalTriL), which is the declaration of the posterior probability distribution structure, in this case a multivariate normal distribution in which only one half of the covariance matrix is estimated (due to symmetry). The total number of parameters in the model is 224 — estimated by variational methods. The deterministic version of this neural network consists of an input layer, ten latent variables (hidden nodes), and an output layer (114 parameters), which does not include the uncertainty in the parameters weights.\n",
    "\n",
    "- model = tfk.Sequential([\n",
    "            tfk.layers.InputLayer(input_shape=(len(inputs),), name=\"input\"),\n",
    "            # just one output (mean, var), so 10->2\n",
    "            tfk.layers.Dense(2, activation=\"relu\", name=\"dense_1\"),\n",
    "            tfk.layers.Dense(tfp.layers.MultivariateNormalTriL.params_size(\n",
    "                len(outputs)), activation=None, name=\"distribution_weights\"),\n",
    "            tfp.layers.MultivariateNormalTriL(len(outputs), \n",
    "                activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior, weight=1/n_batches), name=\"output\")],)\n",
    "\n",
    "#### Aleotoric and Epistemic Uncertainty\n",
    "\n",
    "To account for aleotoric and epistemic uncertainty (uncertainty in parameter weights), the dense layers have to be exchanged with Flipout layers (DenseFlipout) or with Variational layers (DenseVariational). Such a model has more parameters, since every weight is parametrized by normal distribution with non-shared mean and standard deviation, hence doubling the amount of parameter weights. \n",
    "Weights will be resampled for different predictions, and in that case, the Bayesian neural network will act like an ensemble.\n",
    "                \n",
    "- tfp.layers.DenseFlipout(10, activation=\"relu\", name=\"dense_1\")\n",
    "\n",
    "#### Adding uncertainity for the geosciences (bnn3-for-stellarmass)\n",
    "\n",
    "- output_layer = tf.keras.layers.concatenate(\n",
    "        [mu_unit , logsigma_unit , skew_unit , logtau_unit], axis=1)\n",
    "\n",
    "- model = tf.keras.models.Model(inputs=inputs , outputs=output_layer)\n",
    "\n",
    "initialize a single hidden layer\n",
    "- x = tf.keras.layers.Dense( N_HIDDENS,\n",
    "        activation=\"relu\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=RandomNormal(seed=SEED),\n",
    "        kernel_initializer=RandomNormal(seed=SEED), )(x)\n",
    " \n",
    "set final output units separately\n",
    "-  mu_unit = tf.keras.layers.Dense( 1,\n",
    "        activation=\"linear\",\n",
    "        use_bias=True, \n",
    "        bias_initializer=RandomNormal(seed=SEED), \n",
    "        kernel_initializer=RandomNormal(seed=SEED),)(x)\n",
    " \n",
    " - logsigma_unit = tf.keras.layers.Dense( 1,\n",
    "        activation=\"linear\", \n",
    "        use_bias=True, \n",
    "        bias_initializer=Zeros(), \n",
    "        kernel_initializer=Zeros(),)(x)\n",
    " \n",
    " \n",
    " #### Prediction Intervals for Deep Learning Neural Networks (bnn6-for-stellarmass)\n",
    " \n",
    "Next, we can define, train and evaluate a Multilayer Perceptron (MLP) model on the dataset. We will define a simple model with two hidden layers and an output layer that predicts a numeric value. We will use the ReLU activation function and “he” weight initialization, which are a good practice. The number of nodes in each hidden layer was chosen after a little trial and error.\n",
    " \n",
    "- model = tf.keras.Sequential()\n",
    "- model.add(tf.keras.layers.Dense(20, kernel_initializer='he_normal', activation='relu', input_dim=features))\n",
    "- model.add(tf.keras.layers.Dense(5, kernel_initializer='he_normal', activation='relu'))\n",
    "- model.add(tf.keras.layers.Dense(1))\n",
    " \n",
    " #### Simple Bayesian Linear Regression with TensorFlow Probability (bb7-for-stellarmass)\n",
    " \n",
    " - jds_ab = tfd.JointDistributionNamedAutoBatched(\n",
    "        dict(\n",
    "            sigma=tfd.HalfNormal(scale=[tf.cast(1.0, dtype)]),\n",
    "            alpha=tfd.Normal( loc=[tf.cast(0.0, dtype)],  scale=[tf.cast(10.0, dtype)]),\n",
    "            beta=tfd.Normal( \n",
    "                loc=[[tf.cast(0.0, dtype)], [tf.cast(0.0, dtype)]], \n",
    "                scale=[[tf.cast(10.0, dtype)], [tf.cast(10.0, dtype)]]),\n",
    "            y=lambda beta, alpha, sigma: \n",
    "                tfd.Normal( loc=tf.linalg.matmul(x, beta) + alpha, scale=sigma) ))\n",
    "               \n",
    " - uses a Hamiltonian monte carlo to explore the probability distributions\n",
    " \n",
    " It's unclear how this HMC approach relates to the epistemic model probabilities I looked at earlier.\n",
    " \n",
    " But perhaps the fragment presented next is how to think about it.\n",
    " \n",
    " ### Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability\n",
    " \n",
    " Brendan Hasz, 3 Dec 2018, https://brendanhasz.github.io/2018/12/03/tfp-regression.html\n",
    " \n",
    " Variational Bayes\n",
    "\n",
    "Despite its ability to accurately capture uncertainty, one big problem with MCMC sampling is that it’s slow. It took around 20s to sample from the posterior of our simple model with only 100 datapoints! That might not sound too bad, but speed becomes huge when trying to fit models to real (i.e. large) datasets, especially when the model is more complex.\n",
    "\n",
    "To speed things up while still capturing uncertainty, we can make some simplifying assumptions and use stochastic variational inference. These assumptions are that our model’s parameters are completely independent of each other (which MCMC sampling does not assume, since it samples from the joint distribution), and that the posterior for each of our parameters follow some simple distribution (like a normal distribution).\n",
    "\n",
    "How do these assumptions change how we can infer the model parameters? Instead of sampling from the posterior a bunch of times like we did with MCMC (which takes too long), and instead of simply optimizing the parameters (which only gets us a single best point estimate and no uncertainty information), we’ll compromise. **We’ll use stochastic gradient descent to optimize the parameters, but instead of just optimizing the parameter values, we’ll replace each single parameter value with a simple distribution. We’ll replace our point estimates of the weight and bias parameters with normal distributions.** This doubles the numbers of parameters in the model, because now we have a mean parameter and a variance parameter for each single parameter we had before. At each training step, we’ll sample a value from that normal distribution to get the value of the parameter for that training step. Then we can update all the mean and variance parameters (for each original parameter) through backpropagation, instead of updating the point value of the parameter. We’ll also have to use the evidence lower bound (ELBO) loss function, which includes both the loss due to the expected log probability and the difference between the parameter’s distributions and their priors. I’ll do a post on that sometime in the future, but for now see this page.\n",
    "\n",
    "**The advantage of stochastic variational inference over simply optimizing the parameters’ values is that we capture uncertainty as to the model parameter values (because we’re treating each parameter as a distribution now, which has some variance)**. An added bonus is that stochastically sampling from the variational distributions regularizes our model, helping to prevent overfitting. However, while it’s faster than MCMC sampling, it also makes more assumptions than MCMC sampling does (about the shape of the individual parameters’ posteriors and their independence).\n",
    "\n",
    "We’ll use a super-simple one-layer Bayesian neural network with no activation (i.e., a linear regression, which is equivalent to the model we fit with MCMC earlier) and see if it captures the same level of uncertainty as the previous model which we fit with MCMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be226d40-fdd8-4e70-8c3d-1b60afb2e675",
   "metadata": {},
   "source": [
    "## What do we want?\n",
    "\n",
    "The same model that Orduz uses, but with many sigma\n",
    "\n",
    "- jds_ia = tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=0., scale=1.),   # m\n",
    "        tfd.Normal(loc=0., scale=1.),   # b\n",
    "        lambda b, m: tfd.Independent(   # Y\n",
    "            tfd.Normal(loc=m[..., tf.newaxis]*X + b[..., tf.newaxis], \n",
    "            scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]))) ,\n",
    "            reinterpreted_batch_ndims=1)\n",
    "])\n",
    "\n",
    "This looks like 2 guassians for m,n. And then, instead of a single gaussian describing y, it's a series of gaussians.\n",
    "\n",
    "Looks like I just add the prior and posteriors to do the epstemic.\n",
    "\n",
    "but first try this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65bb637b-00b4-4554-b99a-fd264fd35eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x2aac0f278b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "<class 'tensorflow_probability.python.distributions.joint_distribution_auto_batched.JointDistributionNamedAutoBatched'>\n"
     ]
    }
   ],
   "source": [
    "# A linear model. From Geosciences\n",
    "\n",
    "jds_ab = tfd.JointDistributionNamedAutoBatched(dict(\n",
    "\n",
    "    sigma = tfd.HalfNormal(scale=[tf.cast(1.0, dtype)]),\n",
    "\n",
    "    alpha=tfd.Normal(\n",
    "        loc=[tf.cast(3.0, dtype)], \n",
    "        scale=[tf.cast(4.0, dtype)]\n",
    "    ),\n",
    "\n",
    "    beta=tfd.Normal(\n",
    "        loc=[[tf.cast(0.0, dtype)], [tf.cast(0.0, dtype)]], \n",
    "        scale=[[tf.cast(1.0, dtype)], [tf.cast(1.0, dtype)]]\n",
    "    ),\n",
    "\n",
    "    y=lambda beta, alpha, sigma: \n",
    "        tfd.Normal(\n",
    "            loc=tf.linalg.matmul(x, beta) + alpha, \n",
    "            scale=sigma\n",
    "        ) \n",
    "))\n",
    "\n",
    "jds_ab.sample(1)[\"y\"]\n",
    "jds_ab.sample\n",
    "jds_ab.resolve_graph()\n",
    "jds_ab.sample()[\"y\"]\n",
    "print(type(jds_ab))\n",
    "jds_ab.event_shape_tensor()\n",
    "pjds_ab=jds_ab.experimental_pin(y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ce630e-02e6-40fc-9b47-fa5ad5172686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow_probability.python.distributions.joint_distribution_auto_batched.JointDistributionCoroutineAutoBatched'>\n"
     ]
    }
   ],
   "source": [
    "@tfd.JointDistributionCoroutineAutoBatched\n",
    "def model():\n",
    " uranium_weight = yield tfd.Normal(0., scale=1., name='uranium_weight')\n",
    " county_floor_weight = yield tfd.Normal(\n",
    "     0., scale=1., name='county_floor_weight')\n",
    " county_effect = yield tfd.Sample(\n",
    "     tfd.Normal(0., scale=county_effect_scale),\n",
    "     sample_shape=[num_counties], name='county_effect')\n",
    " yield tfd.Normal(\n",
    "     loc=(log_uranium * uranium_weight\n",
    "          + floor_of_house * floor_weight\n",
    "          + floor_by_county * county_floor_weight\n",
    "          + tf.gather(county_effect, county, axis=-1)\n",
    "          + bias),\n",
    "     scale=log_radon_scale[..., tf.newaxis],\n",
    "     name='log_radon')\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03a9f14e-c5af-4c17-ba38-c74928f1927b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow_probability.python.layers.distribution_layer.DistributionLambda'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: <tensorflow_probability.python.layers.distribution_layer.DistributionLambda object at 0x2aac0f40c0d0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53965/1376800537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_graph_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# A Network does not create weights of its own, thus it is already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_validate_graph_inputs_and_outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mcls_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         raise ValueError('Output tensors of a ' + cls_name + ' model must be '\n\u001b[0m\u001b[1;32m    728\u001b[0m                          \u001b[0;34m'the output of a TensorFlow `Layer` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                          '(thus holding past layer metadata). Found: ' + str(x))\n",
      "\u001b[0;31mValueError\u001b[0m: Output tensors of a Functional model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: <tensorflow_probability.python.layers.distribution_layer.DistributionLambda object at 0x2aac0f40c0d0>"
     ]
    }
   ],
   "source": [
    "# A linear model. From Geosciences\n",
    "\n",
    "jds_ab = tfd.JointDistributionNamedAutoBatched(dict(\n",
    "\n",
    "    sigma = tfd.HalfNormal(scale=[tf.cast(1.0, dtype)]),\n",
    "\n",
    "    alpha=tfd.Normal(\n",
    "        loc=[tf.cast(3.0, dtype)], \n",
    "        scale=[tf.cast(4.0, dtype)]\n",
    "    ),\n",
    "\n",
    "    beta=tfd.Normal(\n",
    "        loc=[[tf.cast(0.0, dtype)], [tf.cast(0.0, dtype)]], \n",
    "        scale=[[tf.cast(1.0, dtype)], [tf.cast(1.0, dtype)]]\n",
    "    ),\n",
    "\n",
    "    y=lambda beta, alpha, sigma: \n",
    "        tfd.Normal(\n",
    "            loc=tf.linalg.matmul(x, beta) + alpha, \n",
    "            scale=sigma\n",
    "        ) \n",
    "))\n",
    "hidden_all = tf.keras.layers.Dense(N_Hidden, activation=\"relu\",\n",
    "            use_bias=True, \n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(), \n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(),\n",
    "            name=\"hidden\",\n",
    "         )\n",
    "\n",
    "\n",
    "N_Hidden=10\n",
    "inputs = tf.keras.Input(x.shape[0], name=\"input\") \n",
    "# initialize a single hidden layer\n",
    "hidden = tf.keras.layers.Dense(N_Hidden, activation=\"relu\", use_bias=True, name=\"hidden\",) (inputs)\n",
    "\n",
    "output_layer = tfp.layers.DistributionLambda(\n",
    "    make_distribution_fn=lambda t:pjds_ab(), \n",
    "    convert_to_tensor_fn=lambda s:pjds_ab.sample()[\"y\"])\n",
    "print(type(output_layer))\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a455e36e-0106-4c1b-b39e-c0811ba40f31",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2' has no attribute 'python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47734/425767513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2' has no attribute 'python'"
     ]
    }
   ],
   "source": [
    "tf.python.keras.layers.Lambda(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0539ed-92ba-45ea-a6d9-9d9aae3d2aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('sigma', ()), ('beta', ()), ('alpha', ()), ('y', ('beta', 'alpha', 'sigma')))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jds_ab.resolve_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f7e663c-481a-4c8a-9333-43e7839e35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.base_layer.Layer at 0x2aab013e5ee0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jds_ab.sample()\n",
    "#jds_ab.parameter_properties()\n",
    "#jds_ab.experimental_fit(xc)\n",
    "tf.keras.layers.Layer(jds_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f648030-41a7-4863-a878-094aada6673a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: tfp.distributions.JointDistributionNamedAutoBatched(\"JointDistributionNamedAutoBatched\", batch_shape=[], event_shape={alpha: [1], beta: [2, 1], sigma: [1], y: [163, 1]}, dtype={alpha: float32, beta: float32, sigma: float32, y: float32})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47734/2824147407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjds_ab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m       raise TypeError('The added layer must be '\n\u001b[0m\u001b[1;32m    183\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                       'Found: ' + str(layer))\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: tfp.distributions.JointDistributionNamedAutoBatched(\"JointDistributionNamedAutoBatched\", batch_shape=[], event_shape={alpha: [1], beta: [2, 1], sigma: [1], y: [163, 1]}, dtype={alpha: float32, beta: float32, sigma: float32, y: float32})"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(jds_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a486f07a-cced-4466-83f6-80518f429926",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JointDistributionNamedAutoBatched' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59691/1804698843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Do inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_log_prob_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'JointDistributionNamedAutoBatched' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "def target_log_prob_fn(beta, alpha, sigma):\n",
    "    return jds_ab.log_prob(beta=beta, alpha=alpha, sigma=sigma, y=y)\n",
    "\n",
    "# Do inference.\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=target_log_prob_fn)\n",
    "model.fit(x, y, epochs=1000, verbose=False);\n",
    "\n",
    "# Profit.\n",
    "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
    "yhat = model(x_tst)\n",
    "assert isinstance(yhat, tfd.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0b2f9-ad56-4ff4-a9b3-422bb28ceafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflow",
   "language": "python",
   "name": "tflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
